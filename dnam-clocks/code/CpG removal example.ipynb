{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## An Example of removing generating the original EN, then removing 1% of non-clock CpGs and creating a new EN model.\n",
    " \n",
    "#import the necessary Libraries for this program import pandas as pd\n",
    "from sklearn.linear_model \n",
    "import ElasticNetCV from sklearn.model_selection \n",
    "import train_test_split \n",
    "from sklearn.preprocessing \n",
    "import StandardScaler \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the original dataset\n",
    "hannum_raw = pd.read_pickle('MethylAndAges/Hannum raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the ages associated with Each GSM, and title their Column as Age\n",
    "ages = pd.read_pickle('MethylAndAges/Hannum ages.pkl')\n",
    "ages = ages.rename(columns={ages.columns[0]: 'Age'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose our data such that each row is a GSM. This is needed to allow Standardization with StandardScaler\n",
    "hannum_test = hannum_raw.T hannum_test.columns.name = \"CpG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training and test subsets\n",
    "methyl_raw_train, methyl_raw_test, age_train, age_test = train_test_split(hannum_test, ages, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data such that the fit is to the training set \n",
    "scaler = StandardScaler().fit(methyl_raw_train) methyl_train = scaler.transform(methyl_raw_train)\n",
    "\n",
    "methyl_test = scaler.transform(methyl_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Elastic Net model\n",
    "elastic_netCV_original = ElasticNetCV(l1_ratio = 0.5, n_alphas = 50, cv = 10, \n",
    "                                      n_jobs=1, random_state= 42, max_iter=5000, tol = 0.001, selection='cyclic')\n",
    "#Train the model. \n",
    "elastic_netCV_original.fit(methyl_train, age_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the non-zero coefficients to get the significant cpgs. coeffs_original = pd.DataFrame(elastic_netCV_original.coef_) coeffs_original = coeffs_original[(coeffs_original.T != 0).any()]\n",
    "coeffs_original = coeffs_original.rename(columns={coeffs_original.columns[0]: 'Magnitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get significant CpGs and their indices\n",
    "colnames = pd.DataFrame(hannum_test.columns) sig_cpgs_original = colnames.iloc[coeffs_original.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all non clock CpGs first = True\n",
    "for cpg in sig_cpgs_original[\"CpG\"].to_list():\n",
    "\n",
    "    if first == True:\n",
    "        nonsig_cpgs_original = colnames[colnames[\"CpG\"].str.contains(cpg)==False] \n",
    "        first = False\n",
    "    else:\n",
    "        nonsig_cpgs_original = nonsig_cpgs_original[nonsig_cpgs_original[\"CpG\"].str.contains(cpg)==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find number to be removed\n",
    "num_removed = int(len_nonsig_original*0.01)\n",
    " \n",
    "#Generate CpGs to be removed\n",
    "list_removed = random.sample(range(0, len_nonsig_original), num_removed) \n",
    "cpgs_removed_1 = nonsig_cpgs_original.iloc[list_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a new data set with the randomly selected CpGs removed\n",
    "hannum_test_1 = hannum_test.drop(cpgs_removed_1[\"CpG\"].to_list(), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure that all original clock CpGs remain in the new set\n",
    "common = intersection(hannum_test_1.columns.to_list(), sig_cpgs_original[\"CpG\"].to_list())\n",
    "\n",
    "if not sorted(common) == sorted(sig_cpgs_original[\"CpG\"].to_list()):\n",
    "    raise ValueError('Some Significant CpGs lost!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training and test subsets\n",
    "methyl_raw_train, methyl_raw_test, age_train, age_test = train_test_split(hannum_test_1, ages, test_size=0.2, random_state=42)\n",
    "\n",
    "#Scale our data such that the fit is to the training set \n",
    "scaler = StandardScaler().fit(methyl_raw_train) \n",
    "methyl_train = scaler.transform(methyl_raw_train)\n",
    "methyl_test = scaler.transform(methyl_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Elastic Net model\n",
    "elastic_netCV_1 = ElasticNetCV(l1_ratio = 0.5, n_alphas = 50, cv = 10, n_jobs=11, random_state = 42, max_iter=5000, tol = 0.001, selection='cyclic')\n",
    " \n",
    "#Train the model. \n",
    "elastic_netCV_1.fit(methyl_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an example of the analysis which generate the data for part of figure 3A\n",
    "\n",
    "\n",
    "#import libraries used in this file import pandas as pd\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When given two lists as inputs this function finds all of the common items between the two lists and returns\n",
    "#the results as a list.\n",
    "def intersection(lst1, lst2):\n",
    "lst3 = [value for value in lst1 if value in lst2] return lst3\n",
    "\n",
    "#When given two lists as inputs this function finds all of the items in list1, which are not also in list two, \n",
    "#and returns these items as a list.\n",
    "def loss(lst1, lst2):\n",
    "lst3 = [value for value in lst1 if value not in lst2] return lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we load models with 1-10% of Nonsig removed and find the desired quantities for i in range(1, 11):\n",
    "\n",
    "#Load the Elastic Net model and Dataset with the desired percent of nonsig CpGs removed \n",
    "enet = load('elastic_netCV_Hannum_' + str(i) + '_i5000.joblib')\n",
    "dataset = pd.read_pickle('hannum_' + str(i) + '% nonsig removed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the non-zero coefficients to get the significant cpgs. \n",
    "coeffs_set = pd.DataFrame(enet.coef_)\n",
    "coeffs_set = coeffs_set[(coeffs_set.T != 0).any()]\n",
    " \n",
    "coeffs_set = coeffs_set.rename(columns={coeffs_set.columns[0]: 'Magnitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get significant CpGs and their indices \n",
    "colnames_set = pd.DataFrame(dataset.columns) \n",
    "sig_cpgs_set = colnames_set.iloc[coeffs_set.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find clock CpGs similiar between the original clock and the new dataset\n",
    "common = intersection(sig_cpgs_set[\"CpG\"].to_list(), sig_cpgs_original[\"CpG\"].to_list()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The model with \" + str(i) + \" % of nonsig CpGs removed has \" + str(len(sig_cpgs_set[\"CpG\"].to_list())) + \" Clock CpGs.\")\n",
    "print(\"The model with \" + str(i) + \" % of nonsig CpGs removed has \" + str(len(common)) + \" Clock CpGs in common with the original EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find clock CpGs from the original Clock not included in the new set\n",
    "lost_cpg = loss(sig_cpgs_original[\"CpG\"].to_list(), sig_cpgs_set[\"CpG\"].to_list() )\n",
    "print(\"The model with \" + str(i) + \" % of nonsig CpGs removed does not use \" + str(len(lost_cpg)) + \" of the Clock CpGs in the original model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
